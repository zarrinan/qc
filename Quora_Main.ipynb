{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe, an unsupervised learning algorithm for obtaining vector representations for words\n",
    "wget.download('http://www-nlp.stanford.edu/data/glove.840B.300d.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word2vec pre-trained Google News corpus\n",
    "wget.download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download quora questions train and test data\n",
    "wget.download('http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions strings to lower case and eliminate stop words\n",
    "# apply gensim word2vec model trained on Google News corpus\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert questions strings to lower case and eliminate stop words\n",
    "# apply gensim word2vec model trained Google News corpus with precomputed L2-normalized vectors.\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert words to lower case, tokenize words, remove stop words and leave only alphabetic characters in words\n",
    "# vectorize words, and normalize the result\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data into pandas dataframe\n",
    "data = pd.read_csv('quora-question-pairs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove id columns\n",
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the length of questions strings and calculate the difference\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "data['diff_len'] = data.len_q1 - data.len_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of unique chatacters in each string, exluding the white space\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of words in each string\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of common words in q1 and q2\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using fuzzywuzzy library which compares two strings and outputs a score out of 100, \n",
    "# that denotes two string are equal by giving similarity index\n",
    "\n",
    "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec model and appy to question strings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word2vec model, normalize vectors, and appy to question strings\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [00:02, 186.10it/s]/Users/zarrina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404290it [05:19, 1265.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#Apply sent2vec function to question1_vectors and question2_vectors\n",
    "for i, q in tqdm(enumerate(data.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:00, 1435.20it/s]/Users/zarrina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404290it [03:43, 1812.20it/s]\n"
     ]
    }
   ],
   "source": [
    "question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "# Calculate distances between questions trings\n",
    "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:853: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n"
     ]
    }
   ],
   "source": [
    "data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPickle.dump(question1_vectors, open('q1_w2v.pkl', 'wb'), -1)\n",
    "cPickle.dump(question2_vectors, open('q2_w2v.pkl', 'wb'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('quora_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('quora_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y, tokenize questions\n",
    "\n",
    "y = data.is_duplicate.values\n",
    "\n",
    "tk = text.Tokenizer(num_words=200000)\n",
    "\n",
    "max_len = 40\n",
    "tk.fit_on_texts(list(data.question1.values.astype(str)) + list(data.question2.values.astype(str)))\n",
    "x1 = tk.texts_to_sequences(data.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(embeddings_index, open(\"embeddings.p\", \"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_large = pickle.load( open( \"embeddings.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95596/95596 [00:00<00:00, 294692.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index_large.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:40: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=64, kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:47: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=64, activation=\"relu\", kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:71: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=64, activation=\"relu\", kernel_size=5, strides=1, padding=\"valid\")`\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:78: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=64, activation=\"relu\", kernel_size=5, strides=1, padding=\"valid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/40\n",
      "363861/363861 [==============================] - 1026s 3ms/step - loss: 0.5081 - acc: 0.7487 - val_loss: 0.4515 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.78369, saving model to weights.h5\n",
      "Epoch 2/40\n",
      "363861/363861 [==============================] - 1019s 3ms/step - loss: 0.4055 - acc: 0.8094 - val_loss: 0.4321 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.78369 to 0.79745, saving model to weights.h5\n",
      "Epoch 3/40\n",
      "363861/363861 [==============================] - 1018s 3ms/step - loss: 0.3383 - acc: 0.8450 - val_loss: 0.4487 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79745 to 0.79965, saving model to weights.h5\n",
      "Epoch 4/40\n",
      "363861/363861 [==============================] - 1017s 3ms/step - loss: 0.2807 - acc: 0.8738 - val_loss: 0.4708 - val_acc: 0.7964\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79965\n",
      "Epoch 5/40\n",
      "363861/363861 [==============================] - 1018s 3ms/step - loss: 0.2334 - acc: 0.8959 - val_loss: 0.5129 - val_acc: 0.8027\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79965 to 0.80272, saving model to weights.h5\n",
      "Epoch 6/40\n",
      "363861/363861 [==============================] - 1026s 3ms/step - loss: 0.1953 - acc: 0.9145 - val_loss: 0.5890 - val_acc: 0.8010\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.80272\n",
      "Epoch 7/40\n",
      "363861/363861 [==============================] - 1026s 3ms/step - loss: 0.1664 - acc: 0.9274 - val_loss: 0.6584 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80272\n",
      "Epoch 8/40\n",
      "363861/363861 [==============================] - 1027s 3ms/step - loss: 0.1435 - acc: 0.9384 - val_loss: 0.6994 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80272\n",
      "Epoch 9/40\n",
      "363861/363861 [==============================] - 1025s 3ms/step - loss: 0.1251 - acc: 0.9472 - val_loss: 0.8017 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80272\n",
      "Epoch 10/40\n",
      "363861/363861 [==============================] - 1025s 3ms/step - loss: 0.1096 - acc: 0.9542 - val_loss: 0.8241 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80272 to 0.80583, saving model to weights.h5\n",
      "Epoch 11/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0979 - acc: 0.9597 - val_loss: 0.8828 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80583\n",
      "Epoch 12/40\n",
      "363861/363861 [==============================] - 1025s 3ms/step - loss: 0.0855 - acc: 0.9650 - val_loss: 0.9724 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80583\n",
      "Epoch 13/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0769 - acc: 0.9689 - val_loss: 0.9847 - val_acc: 0.8021\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80583\n",
      "Epoch 14/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0705 - acc: 0.9723 - val_loss: 1.0356 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80583\n",
      "Epoch 15/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0632 - acc: 0.9751 - val_loss: 1.0636 - val_acc: 0.7966\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80583\n",
      "Epoch 16/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0569 - acc: 0.9775 - val_loss: 1.1028 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80583\n",
      "Epoch 17/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0515 - acc: 0.9801 - val_loss: 1.1328 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80583\n",
      "Epoch 18/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0489 - acc: 0.9812 - val_loss: 1.0726 - val_acc: 0.8031\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80583\n",
      "Epoch 19/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0439 - acc: 0.9834 - val_loss: 1.1053 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.80583 to 0.80823, saving model to weights.h5\n",
      "Epoch 20/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0413 - acc: 0.9844 - val_loss: 1.1741 - val_acc: 0.8044\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80823\n",
      "Epoch 21/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0376 - acc: 0.9858 - val_loss: 1.1966 - val_acc: 0.8048\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80823\n",
      "Epoch 22/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0352 - acc: 0.9869 - val_loss: 1.2068 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80823\n",
      "Epoch 23/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0335 - acc: 0.9877 - val_loss: 1.2327 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.80823\n",
      "Epoch 24/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0323 - acc: 0.9880 - val_loss: 1.1931 - val_acc: 0.8070\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80823\n",
      "Epoch 25/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0291 - acc: 0.9896 - val_loss: 1.2874 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80823\n",
      "Epoch 26/40\n",
      "363861/363861 [==============================] - 1021s 3ms/step - loss: 0.0273 - acc: 0.9900 - val_loss: 1.2920 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.80823\n",
      "Epoch 27/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0271 - acc: 0.9902 - val_loss: 1.3003 - val_acc: 0.8018\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.80823\n",
      "Epoch 28/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0264 - acc: 0.9904 - val_loss: 1.2851 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.80823\n",
      "Epoch 29/40\n",
      "363861/363861 [==============================] - 1021s 3ms/step - loss: 0.0250 - acc: 0.9908 - val_loss: 1.2808 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.80823\n",
      "Epoch 30/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0232 - acc: 0.9917 - val_loss: 1.3378 - val_acc: 0.8043\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.80823\n",
      "Epoch 31/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0214 - acc: 0.9922 - val_loss: 1.3547 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.80823\n",
      "Epoch 32/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0222 - acc: 0.9920 - val_loss: 1.2829 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.80823\n",
      "Epoch 33/40\n",
      "363861/363861 [==============================] - 1025s 3ms/step - loss: 0.0203 - acc: 0.9929 - val_loss: 1.3718 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.80823\n",
      "Epoch 34/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0198 - acc: 0.9929 - val_loss: 1.3906 - val_acc: 0.8042\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.80823\n",
      "Epoch 35/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0200 - acc: 0.9928 - val_loss: 1.3371 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.80823\n",
      "Epoch 36/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0191 - acc: 0.9932 - val_loss: 1.3903 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.80823\n",
      "Epoch 37/40\n",
      "363861/363861 [==============================] - 1024s 3ms/step - loss: 0.0180 - acc: 0.9936 - val_loss: 1.3969 - val_acc: 0.8066\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.80823\n",
      "Epoch 38/40\n",
      "363861/363861 [==============================] - 1023s 3ms/step - loss: 0.0172 - acc: 0.9939 - val_loss: 1.3698 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.80823 to 0.80925, saving model to weights.h5\n",
      "Epoch 39/40\n",
      "363861/363861 [==============================] - 1021s 3ms/step - loss: 0.0175 - acc: 0.9939 - val_loss: 1.3939 - val_acc: 0.8093\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.80925 to 0.80932, saving model to weights.h5\n",
      "Epoch 40/40\n",
      "363861/363861 [==============================] - 1022s 3ms/step - loss: 0.0161 - acc: 0.9944 - val_loss: 1.3890 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.80932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff026616ac8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 200000\n",
    "filter_length = 5\n",
    "num_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "input_1 = Input(shape=(40,))\n",
    "embedding_1 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_1)\n",
    "\n",
    "timedistributed_1 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "input_2 = Input(shape=(40,))\n",
    "embedding_2 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_2)\n",
    "\n",
    "timedistributed_2 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "\n",
    "input_3 = Input(shape=(40,))\n",
    "embedding_3 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_3)\n",
    "\n",
    "convolution_3 = Convolution1D(nb_filter=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_3)\n",
    "\n",
    "dropout_3 = Dropout(0.2)(convolution_3)\n",
    "convolution_3_2 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_3)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_3 = GlobalMaxPooling1D()(convolution_3_2)\n",
    "\n",
    "dropout_3_2 = Dropout(0.2)(globalmaxpooling1d_3)\n",
    "\n",
    "dense_3 = Dense(300)(globalmaxpooling1d_3)\n",
    "\n",
    "dropout_3_3 = Dropout(0.2)(dense_3)\n",
    "batchnormalization_3 = BatchNormalization()(dropout_3_3)\n",
    "\n",
    "\n",
    "input_4 = Input(shape=(40,))\n",
    "embedding_4 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_4)\n",
    "\n",
    "convolution_4 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_4)\n",
    "\n",
    "dropout_4 = Dropout(0.2)(convolution_4)\n",
    "convolution_4_2 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_4)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_4 = GlobalMaxPooling1D()(convolution_4_2)\n",
    "\n",
    "dropout_4_2 = Dropout(0.2)(globalmaxpooling1d_4)\n",
    "\n",
    "dense_4 = Dense(300)(globalmaxpooling1d_4)\n",
    "\n",
    "dropout_4_3 = Dropout(0.2)(dense_4)\n",
    "batchnormalization_4 = BatchNormalization()(dropout_4_3)\n",
    "\n",
    "input_5 = Input(shape=(40,))\n",
    "embedding_5 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_5)\n",
    "\n",
    "dropout_5 = Dropout(0.2)(embedding_5)\n",
    "ltsm_5 = LSTM(300)(dropout_5)\n",
    "dropout_5_2 = Dropout(0.2)(ltsm_5)\n",
    "\n",
    "input_6 = Input(shape=(40,))\n",
    "embedding_6 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_6)\n",
    "\n",
    "dropout_6 = Dropout(0.2)(embedding_6)\n",
    "ltsm_6 = LSTM(300)(dropout_6)\n",
    "dropout_6_2 = Dropout(0.2)(ltsm_6)\n",
    "\n",
    "\n",
    "merged = concatenate([lambda_1, \n",
    "                      lambda_2, \n",
    "                      batchnormalization_3, \n",
    "                      batchnormalization_4,\n",
    "                      dropout_5_2,\n",
    "                      dropout_6_2])\n",
    "\n",
    "m_dense = Dense(300)(merged)\n",
    "m_relu = PReLU()(m_dense)\n",
    "m_dropout = Dropout(0.2)(m_relu)\n",
    "m_batch = BatchNormalization()(m_dropout)\n",
    "\n",
    "m2_dense = Dense(300)(m_batch)\n",
    "m2_relu = PReLU()(m2_dense)\n",
    "m2_dropout = Dropout(0.2)(m2_relu)\n",
    "m2_batch = BatchNormalization()(m2_dropout)\n",
    "\n",
    "\n",
    "m3_dense = Dense(300)(m2_batch)\n",
    "m3_relu = PReLU()(m3_dense)\n",
    "m3_dropout = Dropout(0.2)(m3_relu)\n",
    "m3_batch = BatchNormalization()(m3_dropout)\n",
    "\n",
    "m4_dense = Dense(300)(m3_batch)\n",
    "m4_relu = PReLU()(m4_dense)\n",
    "m4_dropout = Dropout(0.2)(m4_relu)\n",
    "m4_batch = BatchNormalization()(m4_dropout)\n",
    "\n",
    "m5_dense = Dense(300)(m4_batch)\n",
    "m5_relu = PReLU()(m_dense)\n",
    "m5_dropout = Dropout(0.2)(m_relu)\n",
    "m5_batch = BatchNormalization()(m5_dropout)\n",
    "\n",
    "\n",
    "dense_out = Dense(1, activation='sigmoid')(m5_batch)\n",
    "\n",
    "# build and compile model\n",
    "model = Model(inputs=[input_1, \n",
    "                      input_2, \n",
    "                      input_3, \n",
    "                      input_4,\n",
    "                      input_5, \n",
    "                      input_6], outputs=[dense_out])\n",
    "\n",
    "model.compile(optimizers.Adam(), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "model.fit([x1, x2, x1, x2, x1, x2 ], y=y, batch_size=384, epochs=40,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the test data into pandas dataframe\n",
    "test = pd.read_csv('test.csv')\n",
    "test = test.drop(['test_id'], axis=1)\n",
    "\n",
    "#Get the length of questions strings and calculate the difference\n",
    "test['len_q1'] = test.question1.apply(lambda x: len(str(x)))\n",
    "test['len_q2'] = test.question2.apply(lambda x: len(str(x)))\n",
    "test['diff_len'] = test.len_q1 - test.len_q2\n",
    "\n",
    "#Get the number of unique chatacters in each string, exluding the white space\n",
    "test['len_char_q1'] = test.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "test['len_char_q2'] = test.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "\n",
    "#Get the number of words in each string\n",
    "test['len_word_q1'] = test.question1.apply(lambda x: len(str(x).split()))\n",
    "test['len_word_q2'] = test.question2.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Find the number of common words in q1 and q2\n",
    "test['common_words'] = test.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "\n",
    "# Using fuzzywuzzy library which compares two strings and outputs a score out of 100, \n",
    "# that denotes two string are equal by giving similarity index\n",
    "\n",
    "test['fuzz_qratio'] = test.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "test['fuzz_WRatio'] = test.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "test['fuzz_partial_ratio'] = test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "test['fuzz_partial_token_set_ratio'] = test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "test['fuzz_token_set_ratio'] = test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "test['fuzz_token_sort_ratio'] = test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "# Build word2vec model and appy to question strings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "test['wmd'] = test.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "# Build word2vec model, normalize vectors, and appy to question strings\n",
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "test['norm_wmd'] = test.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "question1_vectors = np.zeros((test.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "#Apply sent2vec function to question1_vectors and question2_vectors\n",
    "for i, q in tqdm(enumerate(test.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "    \n",
    "question2_vectors  = np.zeros((test.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(test.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)    \n",
    "    \n",
    "    \n",
    "# Calculate distances between questions trings\n",
    "test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "test['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n",
    "\n",
    "test['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "test['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "test['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "test['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n",
    "\n",
    "cPickle.dump(question1_vectors, open('test_q1_w2v.pkl', 'wb'), -1)\n",
    "cPickle.dump(question2_vectors, open('test_q2_w2v.pkl', 'wb'), -1)\n",
    "\n",
    "test.to_csv('quora_test_features.csv', index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test X and y, tokenize questions\n",
    "\n",
    "x1_t = tk.texts_to_sequences(test.question1.values.astype(str))\n",
    "x1_t = sequence.pad_sequences(x1_t, maxlen=max_len)\n",
    "\n",
    "x2_t = tk.texts_to_sequences(test.question2.values.astype(str))\n",
    "x2_t = sequence.pad_sequences(x2_t, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data set predictions\n",
    "predict = model.predict([x1_t, x2_t, x1_t, x2_t, x1_t, x2_t], batch_size=384, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare csv files for submission \n",
    "raw_test = pd.read_csv('test.csv')\n",
    "submission = pd.DataFrame({\"test_id\": raw_test[\"test_id\"], \"is_duplicate\": predict.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map through predictions and apply .50 threshold on prediction values\n",
    "submission_50 = submission.copy()\n",
    "submission_50['is_duplicate'] = [1 if val > .5 else 0 for val in submission_50['is_duplicate']]\n",
    "\n",
    "# Map through predictions and apply .80, .95, .99, and .9999 threshold on prediction values\n",
    "\n",
    "submission_80 = submission.copy()\n",
    "submission_80['is_duplicate'] = [1 if val > .80 else 0 for val in submission_80['is_duplicate']]\n",
    "submission_80.to_csv(\"predictions_80.csv\", index=False)\n",
    "\n",
    "submission_95 = submission.copy()\n",
    "submission_95['is_duplicate'] = [1 if val > .95 else 0 for val in submission_95['is_duplicate']]\n",
    "submission_95.to_csv(\"predictions_95.csv\", index=False)\n",
    "\n",
    "submission_99 = submission.copy()\n",
    "submission_99['is_duplicate'] = [1 if val > .99 else 0 for val in submission_99['is_duplicate']]\n",
    "submission_99.to_csv(\"predictions_99.csv\", index=False)\n",
    "\n",
    "submission_9999 = submission.copy()\n",
    "submission_9999['is_duplicate'] = [1 if val > .9999 else 0 for val in submission_9999['is_duplicate']]\n",
    "submission_9999.to_csv(\"predictions_9999.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
