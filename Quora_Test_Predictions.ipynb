{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import _pickle as cPickle\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('quora_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('quora_test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train X and y, tokenize questions\n",
    "\n",
    "y = data.is_duplicate.values\n",
    "\n",
    "tk = text.Tokenizer(num_words=200000)\n",
    "\n",
    "max_len = 40\n",
    "tk.fit_on_texts(list(data.question1.values.astype(str)) + list(data.question2.values.astype(str)))\n",
    "x1 = tk.texts_to_sequences(data.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2_t, maxlen=max_len)\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test X and y, tokenize questions\n",
    "\n",
    "x1_t = tk.texts_to_sequences(test.question1.values.astype(str))\n",
    "x1_t = sequence.pad_sequences(x1_t, maxlen=max_len)\n",
    "\n",
    "x2_t = tk.texts_to_sequences(test.question2.values.astype(str))\n",
    "x2_t = sequence.pad_sequences(x2_t, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index_large = pickle.load( open( \"embeddings.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index_large.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "filter_length = 5\n",
    "num_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "input_1 = Input(shape=(40,))\n",
    "embedding_1 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_1)\n",
    "\n",
    "timedistributed_1 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "input_2 = Input(shape=(40,))\n",
    "embedding_2 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_2)\n",
    "\n",
    "timedistributed_2 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "\n",
    "input_3 = Input(shape=(40,))\n",
    "embedding_3 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_3)\n",
    "\n",
    "convolution_3 = Convolution1D(nb_filter=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_3)\n",
    "\n",
    "dropout_3 = Dropout(0.2)(convolution_3)\n",
    "convolution_3_2 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_3)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_3 = GlobalMaxPooling1D()(convolution_3_2)\n",
    "\n",
    "dropout_3_2 = Dropout(0.2)(globalmaxpooling1d_3)\n",
    "\n",
    "dense_3 = Dense(300)(globalmaxpooling1d_3)\n",
    "\n",
    "dropout_3_3 = Dropout(0.2)(dense_3)\n",
    "batchnormalization_3 = BatchNormalization()(dropout_3_3)\n",
    "\n",
    "\n",
    "input_4 = Input(shape=(40,))\n",
    "embedding_4 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_4)\n",
    "\n",
    "convolution_4 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_4)\n",
    "\n",
    "dropout_4 = Dropout(0.2)(convolution_4)\n",
    "convolution_4_2 = Convolution1D(filters=num_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_4)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_4 = GlobalMaxPooling1D()(convolution_4_2)\n",
    "\n",
    "dropout_4_2 = Dropout(0.2)(globalmaxpooling1d_4)\n",
    "\n",
    "dense_4 = Dense(300)(globalmaxpooling1d_4)\n",
    "\n",
    "dropout_4_3 = Dropout(0.2)(dense_4)\n",
    "batchnormalization_4 = BatchNormalization()(dropout_4_3)\n",
    "\n",
    "input_5 = Input(shape=(40,))\n",
    "embedding_5 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_5)\n",
    "\n",
    "dropout_5 = Dropout(0.2)(embedding_5)\n",
    "ltsm_5 = LSTM(300)(dropout_5)\n",
    "dropout_5_2 = Dropout(0.2)(ltsm_5)\n",
    "\n",
    "input_6 = Input(shape=(40,))\n",
    "embedding_6 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_6)\n",
    "\n",
    "dropout_6 = Dropout(0.2)(embedding_6)\n",
    "ltsm_6 = LSTM(300)(dropout_6)\n",
    "dropout_6_2 = Dropout(0.2)(ltsm_6)\n",
    "\n",
    "\n",
    "merged = concatenate([lambda_1, \n",
    "                      lambda_2, \n",
    "                      batchnormalization_3, \n",
    "                      batchnormalization_4,\n",
    "                      dropout_5_2,\n",
    "                      dropout_6_2])\n",
    "\n",
    "m_dense = Dense(300)(merged)\n",
    "m_relu = PReLU()(m_dense)\n",
    "m_dropout = Dropout(0.2)(m_relu)\n",
    "m_batch = BatchNormalization()(m_dropout)\n",
    "\n",
    "m2_dense = Dense(300)(m_batch)\n",
    "m2_relu = PReLU()(m2_dense)\n",
    "m2_dropout = Dropout(0.2)(m2_relu)\n",
    "m2_batch = BatchNormalization()(m2_dropout)\n",
    "\n",
    "\n",
    "m3_dense = Dense(300)(m2_batch)\n",
    "m3_relu = PReLU()(m3_dense)\n",
    "m3_dropout = Dropout(0.2)(m3_relu)\n",
    "m3_batch = BatchNormalization()(m3_dropout)\n",
    "\n",
    "m4_dense = Dense(300)(m3_batch)\n",
    "m4_relu = PReLU()(m4_dense)\n",
    "m4_dropout = Dropout(0.2)(m4_relu)\n",
    "m4_batch = BatchNormalization()(m4_dropout)\n",
    "\n",
    "m5_dense = Dense(300)(m4_batch)\n",
    "m5_relu = PReLU()(m_dense)\n",
    "m5_dropout = Dropout(0.2)(m_relu)\n",
    "m5_batch = BatchNormalization()(m5_dropout)\n",
    "\n",
    "\n",
    "dense_out = Dense(1, activation='sigmoid')(m5_batch)\n",
    "\n",
    "# build and compile model\n",
    "model = Model(inputs=[input_1, \n",
    "                      input_2, \n",
    "                      input_3, \n",
    "                      input_4,\n",
    "                      input_5, \n",
    "                      input_6], outputs=[dense_out])\n",
    "\n",
    "model.load_weights(\"weights.h5\")\n",
    "\n",
    "model.compile(optimizers.Adam(), metrics=['accuracy'], loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data set predictions\n",
    "predict = model.predict([x1_t, x2_t, x1_t, x2_t, x1_t, x2_t], batch_size=384, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"test_id\": raw_test[\"test_id\"], \"is_duplicate\": predict.ravel()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map through predictions and apply .50 threshold on prediction values\n",
    "submission_50 = submission.copy()\n",
    "submission_50['is_duplicate'] = [1 if val > .5 else 0 for val in submission_50['is_duplicate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map through predictions and apply .80, .95, .99, and .9999 threshold on prediction values\n",
    "\n",
    "submission_80 = submission.copy()\n",
    "submission_80['is_duplicate'] = [1 if val > .80 else 0 for val in submission_80['is_duplicate']]\n",
    "submission_80.to_csv(\"predictions_80.csv\", index=False)\n",
    "\n",
    "submission_95 = submission.copy()\n",
    "submission_95['is_duplicate'] = [1 if val > .95 else 0 for val in submission_95['is_duplicate']]\n",
    "submission_95.to_csv(\"predictions_95.csv\", index=False)\n",
    "\n",
    "submission_99 = submission.copy()\n",
    "submission_99['is_duplicate'] = [1 if val > .99 else 0 for val in submission_99['is_duplicate']]\n",
    "submission_99.to_csv(\"predictions_99.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_9999 = submission.copy()\n",
    "submission_9999['is_duplicate'] = [1 if val > .9999 else 0 for val in submission_9999['is_duplicate']]\n",
    "submission_9999.to_csv(\"predictions_9999.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
