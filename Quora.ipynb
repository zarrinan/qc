{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glove.840B.300d.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('http://www-nlp.stanford.edu/data/glove.840B.300d.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GoogleNews-vectors-negative300.bin.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "wget.download('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quora_duplicate_questions.tsv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zarrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower() #.decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('quora-question-pairs/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the length of questions strings and calculate the difference\n",
    "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "data['diff_len'] = data.len_q1 - data.len_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of unique chatacters in each string, exluding the white space\n",
    "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of words in each string\n",
    "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the number of common words in q1 and q2\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "275it [00:02, 186.10it/s]/Users/zarrina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404290it [05:19, 1265.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, q in tqdm(enumerate(data.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:00, 1435.20it/s]/Users/zarrina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "404290it [03:43, 1812.20it/s]\n"
     ]
    }
   ],
   "source": [
    "question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:853: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = np.double(unequal_nonzero.sum()) / np.double(nonzero.sum())\n"
     ]
    }
   ],
   "source": [
    "data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zarrina/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:1138: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return l1_diff.sum() / l1_sum.sum()\n"
     ]
    }
   ],
   "source": [
    "data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                          np.nan_to_num(question2_vectors))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['skew_q1vec'] = [skew(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['skew_q2vec'] = [skew(x) for x in np.nan_to_num(question2_vectors)]\n",
    "data['kur_q1vec'] = [kurtosis(x) for x in np.nan_to_num(question1_vectors)]\n",
    "data['kur_q2vec'] = [kurtosis(x) for x in np.nan_to_num(question2_vectors)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cPickle.dump(question1_vectors, open('q1_w2v.pkl', 'wb'), -1)\n",
    "cPickle.dump(question2_vectors, open('q2_w2v.pkl', 'wb'), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('quora_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>...</th>\n",
       "      <th>cityblock_distance</th>\n",
       "      <th>jaccard_distance</th>\n",
       "      <th>canberra_distance</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>minkowski_distance</th>\n",
       "      <th>braycurtis_distance</th>\n",
       "      <th>skew_q1vec</th>\n",
       "      <th>skew_q2vec</th>\n",
       "      <th>kur_q1vec</th>\n",
       "      <th>kur_q2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>5.081614</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94.023324</td>\n",
       "      <td>0.371408</td>\n",
       "      <td>0.168999</td>\n",
       "      <td>0.186557</td>\n",
       "      <td>0.031817</td>\n",
       "      <td>-0.091902</td>\n",
       "      <td>0.050416</td>\n",
       "      <td>0.337301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>-37</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>14.195119</td>\n",
       "      <td>1.0</td>\n",
       "      <td>177.588090</td>\n",
       "      <td>1.012091</td>\n",
       "      <td>0.455910</td>\n",
       "      <td>0.592655</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>0.284010</td>\n",
       "      <td>-0.034444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>9.055989</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.988707</td>\n",
       "      <td>0.666346</td>\n",
       "      <td>0.307828</td>\n",
       "      <td>0.342306</td>\n",
       "      <td>0.239752</td>\n",
       "      <td>0.144554</td>\n",
       "      <td>0.026759</td>\n",
       "      <td>-0.474131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>-15</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>15.987437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>192.237828</td>\n",
       "      <td>1.140536</td>\n",
       "      <td>0.506028</td>\n",
       "      <td>0.692421</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>0.069649</td>\n",
       "      <td>-0.244560</td>\n",
       "      <td>-0.265568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>12.103178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161.408435</td>\n",
       "      <td>0.860225</td>\n",
       "      <td>0.382770</td>\n",
       "      <td>0.480633</td>\n",
       "      <td>-0.133849</td>\n",
       "      <td>0.114777</td>\n",
       "      <td>0.217900</td>\n",
       "      <td>-0.338876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  len_q1  \\\n",
       "0  What is the step by step guide to invest in sh...             0      66   \n",
       "1  What would happen if the Indian government sto...             0      51   \n",
       "2  How can Internet speed be increased by hacking...             0      73   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0      50   \n",
       "4            Which fish would survive in salt water?             0      76   \n",
       "\n",
       "   len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  len_word_q2  \\\n",
       "0      57         9           20           20           14           12   \n",
       "1      88       -37           21           29            8           13   \n",
       "2      59        14           25           24           14           10   \n",
       "3      65       -15           19           26           11            9   \n",
       "4      39        37           25           18           13            7   \n",
       "\n",
       "     ...      cityblock_distance  jaccard_distance  canberra_distance  \\\n",
       "0    ...                5.081614               1.0          94.023324   \n",
       "1    ...               14.195119               1.0         177.588090   \n",
       "2    ...                9.055989               1.0         135.988707   \n",
       "3    ...               15.987437               1.0         192.237828   \n",
       "4    ...               12.103178               1.0         161.408435   \n",
       "\n",
       "   euclidean_distance  minkowski_distance  braycurtis_distance  skew_q1vec  \\\n",
       "0            0.371408            0.168999             0.186557    0.031817   \n",
       "1            1.012091            0.455910             0.592655    0.008735   \n",
       "2            0.666346            0.307828             0.342306    0.239752   \n",
       "3            1.140536            0.506028             0.692421   -0.002527   \n",
       "4            0.860225            0.382770             0.480633   -0.133849   \n",
       "\n",
       "   skew_q2vec  kur_q1vec  kur_q2vec  \n",
       "0   -0.091902   0.050416   0.337301  \n",
       "1    0.094704   0.284010  -0.034444  \n",
       "2    0.144554   0.026759  -0.474131  \n",
       "3    0.069649  -0.244560  -0.265568  \n",
       "4    0.114777   0.217900  -0.338876  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.is_duplicate.values\n",
    "\n",
    "tk = text.Tokenizer(num_words=200000)\n",
    "\n",
    "max_len = 40\n",
    "tk.fit_on_texts(list(data.question1.values.astype(str)) + list(data.question2.values.astype(str)))\n",
    "x1 = tk.texts_to_sequences(data.question1.values.astype(str))\n",
    "x1 = sequence.pad_sequences(x1, maxlen=max_len)\n",
    "\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2, maxlen=max_len)\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [04:45, 7696.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.840B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95596/95596 [06:26<00:00, 247.55it/s] \n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "filter_length = 5\n",
    "num_filter = 64\n",
    "pool_length = 4\n",
    "\n",
    "input_1 = Input(shape=(40,))\n",
    "embedding_1 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_1)\n",
    "\n",
    "timedistributed_1 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "input_2 = Input(shape=(40,))\n",
    "embedding_2 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_2)\n",
    "\n",
    "timedistributed_2 = TimeDistributed(Dense(300, activation='relu'))(embedding_1)\n",
    "lambda_2 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(300,))(timedistributed_1)\n",
    "\n",
    "\n",
    "\n",
    "input_3 = Input(shape=(40,))\n",
    "embedding_3 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_3)\n",
    "\n",
    "convolution_3 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_3)\n",
    "\n",
    "dropout_3 = Dropout(0.2)(convolution_3)\n",
    "convolution_3_2 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_3)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_3 = GlobalMaxPooling1D()(convolution_3_2)\n",
    "\n",
    "dropout_3_2 = Dropout(0.2)(globalmaxpooling1d_3)\n",
    "\n",
    "dense_3 = Dense(300)(globalmaxpooling1d_3)\n",
    "\n",
    "dropout_3_3 = Dropout(0.2)(dense_3)\n",
    "batchnormalization_3 = BatchNormalization()(dropout_3_3)\n",
    "\n",
    "\n",
    "input_4 = Input(shape=(40,))\n",
    "embedding_4 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=40,\n",
    "                        trainable=False)(input_4)\n",
    "\n",
    "convolution_4 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(embedding_4)\n",
    "\n",
    "dropout_4 = Dropout(0.2)(convolution_4)\n",
    "convolution_4_2 = Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1)(dropout_4)\n",
    "\n",
    "\n",
    "globalmaxpooling1d_4 = GlobalMaxPooling1D()(convolution_4_2)\n",
    "\n",
    "dropout_4_2 = Dropout(0.2)(globalmaxpooling1d_4)\n",
    "\n",
    "dense_4 = Dense(300)(globalmaxpooling1d_4)\n",
    "\n",
    "dropout_4_3 = Dropout(0.2)(dense_4)\n",
    "batchnormalization_4 = BatchNormalization()(dropout_4_3)\n",
    "\n",
    "input_5 = Input(shape=(40,))\n",
    "embedding_5 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_5)\n",
    "\n",
    "dropout_5 = Dropout(0.2)(embedding_5)\n",
    "ltsm_5 = LSTM(300)(dropout_5)\n",
    "dropout_5_2 = Dropout(0.2)(ltsm_5)\n",
    "\n",
    "input_6 = Input(shape=(40,))\n",
    "embedding_6 = Embedding(input_dim=len(word_index) + 1,\n",
    "                        output_dim=300,\n",
    "                       input_length=40)(input_6)\n",
    "\n",
    "dropout_6 = Dropout(0.2)(embedding_6)\n",
    "ltsm_6 = LSTM(300)(dropout_6)\n",
    "dropout_6_2 = Dropout(0.2)(ltsm_6)\n",
    "\n",
    "\n",
    "merged = concatenate([lambda_1, \n",
    "                      lambda_2, \n",
    "                      batchnormalization_3, \n",
    "                      batchnormalization_4,\n",
    "                      dropout_5_2,\n",
    "                      dropout_6_2])\n",
    "\n",
    "m_dense = Dense(300)(merged)\n",
    "m_relu = PReLU()(m_dense)\n",
    "m_dropout = Dropout(0.2)(m_relu)\n",
    "m_batch = BatchNormalization()(m_dropout)\n",
    "\n",
    "m2_dense = Dense(300)(m_batch)\n",
    "m2_relu = PReLU()(m2_dense)\n",
    "m2_dropout = Dropout(0.2)(m2_relu)\n",
    "m2_batch = BatchNormalization()(m2_dropout)\n",
    "\n",
    "\n",
    "m3_dense = Dense(300)(m2_batch)\n",
    "m3_relu = PReLU()(m3_dense)\n",
    "m3_dropout = Dropout(0.2)(m3_relu)\n",
    "m3_batch = BatchNormalization()(m3_dropout)\n",
    "\n",
    "m4_dense = Dense(300)(m3_batch)\n",
    "m4_relu = PReLU()(m4_dense)\n",
    "m4_dropout = Dropout(0.2)(m4_relu)\n",
    "m4_batch = BatchNormalization()(m4_dropout)\n",
    "\n",
    "m5_dense = Dense(300)(m4_batch)\n",
    "m5_relu = PReLU()(m_dense)\n",
    "m5_dropout = Dropout(0.2)(m_relu)\n",
    "m5_batch = BatchNormalization()(m5_dropout)\n",
    "\n",
    "\n",
    "dense_out = Dense(1, activation='sigmoid')(m5_batch)\n",
    "\n",
    "# build and compile model\n",
    "model = Model(inputs=[input_1, \n",
    "                      input_2, \n",
    "                      input_3, \n",
    "                      input_4,\n",
    "                      input_5, \n",
    "                      input_6], outputs=[dense_out])\n",
    "\n",
    "model.compile(optimizers.Adam(), metrics=['accuracy'], loss='binary_crossentropy')\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "model.fit([x1, x2, x1, x2, x1, x2 ], y=y, batch_size=384, epochs=1,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
